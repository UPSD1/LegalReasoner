#!/bin/bash
# Legal Dataset GRPO Training - 1 GPU Setup with OpenAI GPT-4o-mini Model-Based Rewards
# Using your proven working structure with model-based reward function

set -e

echo "‚öñÔ∏è Legal Dataset GRPO Training - 1 GPU Setup with GPT-4o-mini Rewards"
echo "==================================================================="

# Step 0: Check OpenAI API key
echo "üîë Step 0: OpenAI API Setup"
if [ -z "$OPENAI_API_KEY" ]; then
    echo "‚ùå OPENAI_API_KEY environment variable not set!"
    echo "üí° Set it with: export OPENAI_API_KEY='your-api-key-here'"
    echo "üí° Or add it to your ~/.bashrc"
    exit 1
else
    echo "‚úÖ OpenAI API key found"
    # Test API connection with new Responses API
    python3 -c "
import openai
import os
try:
    client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    # Test with the new Responses API (March 2025)
    response = client.responses.create(
        model='gpt-4o-mini',
        input='Hello',
        max_output_tokens=20
    )
    print('‚úÖ OpenAI GPT-4o-mini Responses API connection successful')
except Exception as e:
    print(f'‚ùå OpenAI Responses API test failed: {e}')
    exit(1)
"
fi

# Step 1: Environment setup (same as your working version)
echo "üìã Step 1: Environment Setup"
export CUDA_VISIBLE_DEVICES=0,1,2,3
export PYTHONUNBUFFERED=1

# Step 2: Create legal dataset (same structure, but modified for model-based rewards)
echo "‚öñÔ∏è Step 2: Creating Legal Dataset for Model-Based Rewards"
python3 -c "
import pandas as pd
from pathlib import Path
import json

def create_legal_dataset_for_model_rewards():
    '''
    Create legal dataset optimized for model-based similarity evaluation
    Uses proper Verl structure for custom reward functions
    '''
    
    legal_problems = [
        {
            'question': 'What is the minimum age to serve as President of the United States?',
            'answer': 'According to Article II of the Constitution, a person must be at least 35 years old to serve as President of the United States.',
            'difficulty': 'easy',
            'legal_area': 'constitutional_law',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        },
        {
            'question': 'How many justices currently serve on the U.S. Supreme Court?',
            'answer': 'The U.S. Supreme Court consists of nine justices: one Chief Justice and eight Associate Justices.',
            'difficulty': 'easy', 
            'legal_area': 'constitutional_law',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        },
        {
            'question': 'What is the statute of limitations for most federal crimes?',
            'answer': 'Most federal crimes have a statute of limitations of five years, meaning prosecution must begin within five years of when the crime was committed.',
            'difficulty': 'easy',
            'legal_area': 'criminal_law',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        },
        {
            'question': 'In contract law, what are the essential elements required for a valid contract?',
            'answer': 'A valid contract requires: (1) offer, (2) acceptance, (3) consideration (exchange of value), (4) legal capacity of parties, and (5) legal purpose.',
            'difficulty': 'medium',
            'legal_area': 'contract_law',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        },
        {
            'question': 'Under federal law, how much advance notice must employers give for mass layoffs under the WARN Act?',
            'answer': 'The Worker Adjustment and Retraining Notification (WARN) Act requires employers to provide 60 days advance written notice before mass layoffs.',
            'difficulty': 'medium',
            'legal_area': 'employment_law',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        },
        {
            'question': 'What is the duration of protection for a utility patent in the United States?',
            'answer': 'Utility patents in the United States provide protection for 20 years from the date the patent application was filed.',
            'difficulty': 'medium',
            'legal_area': 'intellectual_property',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        },
        {
            'question': 'In bankruptcy law, how long must an individual wait before filing for Chapter 7 bankruptcy again?',
            'answer': 'An individual must wait 8 years (96 months) from the date of a previous Chapter 7 discharge before filing for Chapter 7 bankruptcy again.',
            'difficulty': 'medium',
            'legal_area': 'bankruptcy_law',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        },
        {
            'question': 'Under federal sentencing guidelines, what is the purpose of criminal history points?',
            'answer': 'Criminal history points under federal sentencing guidelines are used to determine a defendant\\'s criminal history category, which affects the sentencing range. The maximum is 13 points.',
            'difficulty': 'medium',
            'legal_area': 'criminal_law',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        },
        {
            'question': 'In securities law, how quickly must corporate insiders report their stock transactions?',
            'answer': 'Under Section 16 of the Securities Exchange Act, corporate insiders must report their stock transactions within two business days.',
            'difficulty': 'hard',
            'legal_area': 'securities_law',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        },
        {
            'question': 'What is the minimum number of directors required for a Delaware corporation?',
            'answer': 'Delaware law requires that a corporation have at least one director on its board of directors.',
            'difficulty': 'easy',
            'legal_area': 'corporate_law',
            'task_type': 'general_chat',
            'jurisdiction': 'california',
        }
    ]
    
    # Convert to Verl format for custom reward functions
    verl_dataset = []
    
    for idx, problem in enumerate(legal_problems):
        data_item = {
            # CRITICAL: Use custom data_source for custom reward function
            'data_source': 'custom_legal',
            
            # Prompt in chat format
            'prompt': [{'role': 'user', 'content': problem['question']}],
            
            # For custom reward functions, ground_truth goes in reward_model field
            'reward_model': {
                'style': 'rule',
                'ground_truth': problem['answer']
            },
            
            # Metadata for custom reward function
            'ability': 'legal_reasoning',
            'difficulty': problem['difficulty'],
            'legal_area': problem['legal_area'],
            'extra_info': {
                'split': 'train' if idx < 8 else 'test',
                'index': idx,
                'domain': 'legal',
                'reward_type': 'model_based',
                'task_type': 'general_chat',
                'jurisdiction': 'california',
                'legal_area': problem['legal_area'],
                'prompt': problem['question']  # Include for reward function
            }
        }
        verl_dataset.append(data_item)
    
    return verl_dataset

# Create the dataset
dataset = create_legal_dataset_for_model_rewards()

# Split into train/test
train_data = dataset[:8]
test_data = dataset[8:]

# Save as parquet
output_dir = Path.home() / 'data' / 'legal_model_rewards'
output_dir.mkdir(parents=True, exist_ok=True)

train_df = pd.DataFrame(train_data)
test_df = pd.DataFrame(test_data)

train_df.to_parquet(output_dir / 'train.parquet', index=False)
test_df.to_parquet(output_dir / 'test.parquet', index=False)

print(f'‚úÖ Legal dataset for model-based rewards created:')
print(f'   üìÅ Train: {len(train_df)} samples')
print(f'   üìÅ Test: {len(test_df)} samples')
print(f'   ü§ñ Will use GPT-4o-mini for answer similarity evaluation')

print(f'\\nüìã Sample Questions:')
for i, item in enumerate(train_data[:3]):
    question = item['prompt'][0]['content']
    expected = item['reward_model']['ground_truth']
    area = item['legal_area']
    print(f'   {i+1}. [{area}] {question}')
    print(f'      Expected: {expected[:80]}...')
"

# Step 3: Model check (same as before)
echo "ü§ñ Step 3: Model Check"
python3 -c "
import transformers
try:
    tokenizer = transformers.AutoTokenizer.from_pretrained('Qwen/Qwen2.5-1.5B-Instruct')
    print('‚úÖ Model ready: Qwen2.5-1.5B-Instruct')
except Exception as e:
    print(f'‚ùå Model error: {e}')
"

# Step 4: Legal Dataset GRPO Training with GPT-4o-mini Rewards
echo "‚öñÔ∏è Step 4: Legal Dataset GRPO Training with GPT-4o-mini Model-Based Rewards"
echo ""
echo "üîß CONFIGURATION:"
echo "  ‚Ä¢ Same proven batch sizes (8/8/1)"
echo "  ‚Ä¢ Same GRPO settings (group sampling)"
echo "  ‚Ä¢ Same 1-GPU setup"
echo "  ‚Ä¢ NEW: GPT-4o-mini model-based reward function"
echo "  ‚Ä¢ NEW: Similarity-based evaluation (0-1 scale)"
echo ""

TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_FILE="legal_grpo_gpt4_rewards_${TIMESTAMP}.log"

echo "üìù Training log: $LOG_FILE"
echo "‚è∞ Starting legal GRPO training with GPT-4o-mini rewards..."

# Modified configuration for model-based rewards with correct Verl custom reward function setup
PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \
algorithm.adv_estimator=grpo \
data.train_files=$HOME/data/legal_model_rewards/train.parquet \
data.val_files=$HOME/data/legal_model_rewards/test.parquet \
data.train_batch_size=4 \
data.max_prompt_length=256 \
data.max_response_length=512 \
actor_rollout_ref.model.path=Qwen/Qwen2.5-1.5B-Instruct \
actor_rollout_ref.actor.optim.lr=1e-6 \
actor_rollout_ref.actor.ppo_mini_batch_size=4 \
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
actor_rollout_ref.actor.use_kl_loss=True \
actor_rollout_ref.actor.kl_loss_coef=0.001 \
actor_rollout_ref.actor.kl_loss_type=low_var_kl \
actor_rollout_ref.rollout.n=2 \
actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=2 \
actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \
actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=1 \
reward_model.enable=false \
custom_reward_function.path=verl_integration.py \
custom_reward_function.name=multi_task_legal_reward_function \
trainer.logger='["console"]' \
trainer.project_name=legal_gpt4_rewards \
trainer.experiment_name=legal_gpt4_${TIMESTAMP} \
trainer.n_gpus_per_node=1 \
trainer.nnodes=1 \
trainer.save_freq=2 \
trainer.test_freq=1 \
trainer.total_epochs=3 \
trainer.val_before_train=False 2>&1 | tee $LOG_FILE

# Check results
if [ $? -eq 0 ]; then
    echo ""
    echo "‚öñÔ∏è SUCCESS! Legal GRPO Training with GPT-4o-mini Rewards Completed!"
    echo "================================================================="
    echo ""
    echo "‚úÖ LEGAL REASONING MODEL TRAINED WITH AI EVALUATION:"
    echo "  ‚Ä¢ 8 legal training questions across multiple domains"
    echo "  ‚Ä¢ GPT-4o-mini evaluates answer similarity (0-1 scale)"
    echo "  ‚Ä¢ Model learns from AI feedback on answer quality"
    echo "  ‚Ä¢ Captures semantic similarity, not just exact matches"
    echo ""
    echo "ü§ñ GPT-4o-mini Evaluation Features:"
    echo "  ‚Ä¢ Understands legal concept similarity"
    echo "  ‚Ä¢ Evaluates factual accuracy"
    echo "  ‚Ä¢ Considers completeness of answers"
    echo "  ‚Ä¢ Provides nuanced 0-1 scoring"
    echo ""
    echo "üéØ Advantages over Rule-Based Rewards:"
    echo "  ‚Ä¢ Handles paraphrased correct answers"
    echo "  ‚Ä¢ Understands conceptual equivalence"
    echo "  ‚Ä¢ Evaluates partial correctness"
    echo "  ‚Ä¢ More realistic assessment"
    echo ""
    echo "üíæ Model saved: checkpoints/legal_gpt4_rewards/legal_gpt4_${TIMESTAMP}/"
    echo "üìù Training log: $LOG_FILE"
    
fi

echo ""
echo "üìö MODEL-BASED REWARD SUMMARY (UPDATED)"
echo "======================================"
echo ""
echo "ü§ñ OpenAI Responses API Integration (March 2025):"
echo "   ‚Ä¢ Uses NEW client.responses.create() API endpoint"
echo "   ‚Ä¢ Simplified input parameter (no more messages array)"
echo "   ‚Ä¢ More efficient for single evaluations"
echo "   ‚Ä¢ Built for agentic applications"
echo ""
echo "‚öñÔ∏è Correct Verl Implementation:"
echo "   ‚Ä¢ Custom data_source: 'custom_legal' (not 'openai/gsm8k')"
echo "   ‚Ä¢ Proper reward function signature: compute_score(data_source, solution_str, ground_truth, extra_info)"
echo "   ‚Ä¢ Correct config: custom_reward_function.path and custom_reward_function.name"
echo "   ‚Ä¢ Dataset structure: reward_model.ground_truth field"
echo ""
echo "üîÑ Training Process:"
echo "   1. Model generates legal answers"
echo "   2. GPT-4o-mini compares with expected answers using Responses API" 
echo "   3. Similarity scores (0-1) used as rewards"
echo "   4. GRPO updates model based on AI evaluation"
echo ""
echo "üéØ Key Technical Improvements:"
echo "   ‚Ä¢ Fixed OpenAI API usage (Responses API vs deprecated Chat Completions)"
echo "   ‚Ä¢ Correct Verl custom reward function implementation"
echo "   ‚Ä¢ Proper dataset structure for custom data sources"
echo "   ‚Ä¢ Efficient API calls with rate limiting"
echo ""
echo "üí° Cost & Performance:"
echo "   ‚Ä¢ GPT-4o-mini Responses API: ~$0.0015 per evaluation"
echo "   ‚Ä¢ Faster evaluation with new API endpoint"
echo "   ‚Ä¢ Better error handling and fallback mechanisms"
echo "   ‚Ä¢ Reduced batch size to manage API costs"
echo ""
echo "‚öñÔ∏è You now have a legal model trained with CORRECT AI-based evaluation! ‚öñÔ∏è"